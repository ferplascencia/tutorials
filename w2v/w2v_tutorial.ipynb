{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast version? (1 for yes) 1\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import gensim\n",
    "from random import sample\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "print('Fast version? (1 for yes) {}'.format(gensim.models.word2vec.FAST_VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plan\n",
    "1. Class to read in data\n",
    "1. Train word embedding model\n",
    "1. Get ground truth\n",
    "1. Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReviewGetter(object):\n",
    "    \"\"\"\n",
    "    This class iterates through reviews\n",
    "    \n",
    "    - Store input paths on the class\n",
    "    \n",
    "    - The __iter__ method, each time called, will run through the docs again\n",
    "      We need to iterate a bunch of times so this makes sense\n",
    "    \n",
    "    - On the first pass, we save the ground truth labels on the class\n",
    "    \n",
    "    - Also on first pass, we save word counts\n",
    "      Word probabilities will be important later\n",
    "    \"\"\"\n",
    "    BASE_FOLDER = 'review_polarity/txt_sentoken'\n",
    "    PATHS = {\n",
    "        'pos': glob.glob(BASE_FOLDER + '/pos/*.txt'),\n",
    "        'neg': glob.glob(BASE_FOLDER + '/neg/*.txt'),\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.n_reviews = 0\n",
    "        self.word_count_dict = {}\n",
    "        self.first_run = True\n",
    "    \n",
    "    def get_reviews(self):\n",
    "        \"\"\"\n",
    "        - Will iterate through the docs each time called\n",
    "        \n",
    "        - Yields a list of tokens\n",
    "        - We do one preprocessing step,\n",
    "          adding <b> at the beginning and </b> at the end of a review\n",
    "          plus adding <s> between sentences\n",
    "        \n",
    "        Example use:\n",
    "        r = ReviewIterator()\n",
    "        for review in r:\n",
    "            do_stuff(review)\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "\n",
    "        reviews = []\n",
    "        \n",
    "        for label, paths in self.PATHS.items():\n",
    "            if label == 'pos':\n",
    "                valence = 1\n",
    "            else:\n",
    "                valence = 0\n",
    "            for path in paths:             \n",
    "                with open(path, 'r') as infile:\n",
    "                    review = []\n",
    "                    for line in infile:\n",
    "                        review.extend(line.split() + ['<s>'])\n",
    "                        # bookkeeping\n",
    "                    count += 1\n",
    "                    if count % 500 == 0:\n",
    "                        logging.info('Yielded {} reviews'.format(count))\n",
    "                    if self.first_run == True:\n",
    "                        self.n_reviews += 1\n",
    "                        for word in review:\n",
    "                            try:\n",
    "                                self.word_count_dict[word] += 1\n",
    "                            except:\n",
    "                                self.word_count_dict[word] = 1\n",
    "                    reviews.append((valence, review))\n",
    "        self.first_run = False\n",
    "        return reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-03 13:19:27,971 : INFO : Yielded 500 reviews\n",
      "2017-05-03 13:19:28,241 : INFO : Yielded 1000 reviews\n",
      "2017-05-03 13:19:28,429 : INFO : Yielded 1500 reviews\n",
      "2017-05-03 13:19:28,610 : INFO : Yielded 2000 reviews\n"
     ]
    }
   ],
   "source": [
    "r = ReviewGetter()\n",
    "\n",
    "data = r.get_reviews()\n",
    "\n",
    "reviews = [x[1] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['every', 'now', 'and', 'then', 'a', 'movie', 'comes', 'along', 'from', 'a', 'suspect', 'studio', ',', 'with', 'every', 'indication', 'that', 'it', 'will', 'be', 'a', 'stinker', ',', 'and', 'to', \"everybody's\", 'surprise', '(', 'perhaps', 'even', 'the', 'studio', ')', 'the', 'film', 'becomes', 'a', 'critical', 'darling', '.', '<s>', 'mtv', \"films'\", '_election', ',', 'a', 'high', 'school', 'comedy', 'starring']\n"
     ]
    }
   ],
   "source": [
    "len(reviews)\n",
    "print(reviews[1][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec happens here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-03 13:19:31,098 : INFO : collecting all words and their counts\n",
      "2017-05-03 13:19:31,099 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-03 13:19:31,429 : INFO : collected 50921 word types from a corpus of 1557401 raw words and 2000 sentences\n",
      "2017-05-03 13:19:31,430 : INFO : Loading a fresh vocabulary\n",
      "2017-05-03 13:19:31,512 : INFO : min_count=5 retains 15284 unique words (30% of original 50921, drops 35637)\n",
      "2017-05-03 13:19:31,513 : INFO : min_count=5 leaves 1499751 word corpus (96% of original 1557401, drops 57650)\n",
      "2017-05-03 13:19:31,559 : INFO : deleting the raw counts dictionary of 50921 items\n",
      "2017-05-03 13:19:31,561 : INFO : sample=0.0001 downsamples 353 most-common words\n",
      "2017-05-03 13:19:31,562 : INFO : downsampling leaves estimated 683187 word corpus (45.6% of prior 1499751)\n",
      "2017-05-03 13:19:31,563 : INFO : estimated required memory for 15284 words and 100 dimensions: 19869200 bytes\n",
      "2017-05-03 13:19:31,617 : INFO : resetting layer weights\n",
      "2017-05-03 13:19:31,806 : INFO : training model with 2 workers on 15284 vocabulary and 100 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-05-03 13:19:32,809 : INFO : PROGRESS: at 3.75% examples, 769649 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:33,815 : INFO : PROGRESS: at 7.47% examples, 763987 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:34,819 : INFO : PROGRESS: at 10.72% examples, 730655 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:35,829 : INFO : PROGRESS: at 14.07% examples, 718157 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:36,831 : INFO : PROGRESS: at 17.36% examples, 709030 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:37,833 : INFO : PROGRESS: at 20.86% examples, 710033 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:38,837 : INFO : PROGRESS: at 24.41% examples, 712192 words/s, in_qsize 4, out_qsize 1\n",
      "2017-05-03 13:19:39,837 : INFO : PROGRESS: at 27.99% examples, 715270 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:40,845 : INFO : PROGRESS: at 31.41% examples, 712904 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:41,848 : INFO : PROGRESS: at 34.60% examples, 706822 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:42,850 : INFO : PROGRESS: at 38.45% examples, 714087 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:43,855 : INFO : PROGRESS: at 42.06% examples, 715759 words/s, in_qsize 4, out_qsize 0\n",
      "2017-05-03 13:19:44,855 : INFO : PROGRESS: at 45.73% examples, 718702 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:45,858 : INFO : PROGRESS: at 49.22% examples, 718168 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:46,859 : INFO : PROGRESS: at 52.73% examples, 717974 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:47,865 : INFO : PROGRESS: at 56.15% examples, 716595 words/s, in_qsize 4, out_qsize 0\n",
      "2017-05-03 13:19:48,868 : INFO : PROGRESS: at 59.82% examples, 718459 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:49,875 : INFO : PROGRESS: at 63.39% examples, 718922 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:50,879 : INFO : PROGRESS: at 66.86% examples, 718434 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:51,884 : INFO : PROGRESS: at 70.44% examples, 719015 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:52,885 : INFO : PROGRESS: at 74.31% examples, 722571 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:53,886 : INFO : PROGRESS: at 77.47% examples, 719074 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:54,893 : INFO : PROGRESS: at 81.04% examples, 719441 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:55,898 : INFO : PROGRESS: at 84.68% examples, 720578 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:56,901 : INFO : PROGRESS: at 88.41% examples, 722210 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:57,907 : INFO : PROGRESS: at 92.20% examples, 724098 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:58,909 : INFO : PROGRESS: at 95.80% examples, 724548 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:19:59,913 : INFO : PROGRESS: at 99.01% examples, 722066 words/s, in_qsize 3, out_qsize 0\n",
      "2017-05-03 13:20:00,268 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-03 13:20:00,269 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-03 13:20:00,269 : INFO : training on 46722030 raw words (20490992 effective words) took 28.5s, 719963 effective words/s\n"
     ]
    }
   ],
   "source": [
    "w2v_model = gensim.models.Word2Vec(\n",
    "    sample(reviews, len(reviews)),\n",
    "    iter=30, # number of algorithm iterations\n",
    "    seed=42,\n",
    "    workers=2, # number of CPU cores to use\n",
    "    size=100, # length of each word vector\n",
    "    sample=1e-4, # if word has p>sample, downsample it\n",
    "    window=10, # window on sides of focal word\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V diagnostics\n",
    "\n",
    "We should pick some words and make sure their synonyms make sense to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-03 13:20:06,051 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('quentin', 0.7592371106147766),\n",
       " ('masterpiece', 0.6866481304168701),\n",
       " (\"spielberg's\", 0.6368312835693359),\n",
       " ('fiction', 0.6258630752563477),\n",
       " ('jules', 0.6246006488800049),\n",
       " (\"tarantino's\", 0.6116619110107422),\n",
       " ('pulp', 0.6040959358215332),\n",
       " ('directing', 0.6025127172470093),\n",
       " ('zwick', 0.6019469499588013),\n",
       " ('masses', 0.6017510294914246)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['woman'])\n",
    "w2v_model.wv.most_similar(positive=['man'], negative=['king'])\n",
    "w2v_model.wv.most_similar(positive=['spielberg'])\n",
    "w2v_model.wv.most_similar(positive=['spielberg', 'tarantino'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine to make review vectors\n",
    "\n",
    "We use a really simple method:\n",
    "1. Choose an `alpha` level (lit indicates 0.001 to 0.0001, but for our task a value of 1 works best)\n",
    "1. For each word a person uses, get the word vector `v_word`\n",
    "1. Weight `v_word` by its overall frequency in the corpus `p_word` using formula `alpha / (alpha + p_word)` to get `v_word_weighted`. This downweights the impact of common words. A smaller `alpha` makes this downweighting more severe.\n",
    "1. Add `v_word_weighted` to the user vector `v_user` (which we initialize as all 0's.\n",
    "1. To account for different activity levels of different users, after we are done adding `v_word_weighted` for all words a user has used, we divide `v_user` by `n_words_user` to get our final user vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = w2v_model.wv\n",
    "\n",
    "word_vocab_dict = w2v_model.wv.vocab\n",
    "word_count_dict = r.word_count_dict\n",
    "\n",
    "word_prob_dict = {k: word_count_dict[k] for k in word_vocab_dict.keys()}\n",
    "word_count_sum = sum(word_prob_dict.values())\n",
    "word_prob_dict = {k: v / word_count_sum for k, v in word_prob_dict.items()}\n",
    "\n",
    "review_token_counts = np.zeros((2000,))\n",
    "review_vectors = np.zeros((2000, 100))\n",
    "\n",
    "a = 1\n",
    "\n",
    "for idx, (label, review_tokens) in enumerate(data):\n",
    "    for token in review_tokens:\n",
    "        if token in word_vectors:\n",
    "            p = word_prob_dict[token]\n",
    "            review_token_counts[idx] += 1\n",
    "            review_vectors[idx] += (a / (a + p)) * word_vectors[token]\n",
    "\n",
    "review_vectors = review_vectors / review_token_counts[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and evaluate\n",
    "\n",
    "### Model\n",
    "\n",
    "Logistic regression with L2 penalty (\"Ridge\").\n",
    "\n",
    "This penalizes the L2-norm of the coefficient vector.\n",
    "\n",
    "L2 logistic regression is the most common way I've seen of summarizing word vectors. They seem to do well with \"dense vectors\" (e.g. not many 0's).\n",
    "\n",
    "### Metrics\n",
    "\n",
    "1. Accuracy: What fraction of predictions are correct?\n",
    "1. Precision: What fraction of classified 1's are true 1's?\n",
    "1. Recall: What fraction of true 1's are actually predicted 1's from the classifier?\n",
    "1. F1: Harmonic mean of Precision and Recall (`2 * (p*r) / (p + r)`)\n",
    "1. AUC: Area under the receiver operating characteristic (ROC) curve. We show a plot below because this is an important measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.8305\n",
      "prec = 0.8352092557309087\n",
      "rec = 0.8228884245924271\n",
      "f1 = 0.8289623758992128\n",
      "auc = 0.9136201030879427\n"
     ]
    }
   ],
   "source": [
    "X = review_vectors\n",
    "y = np.array([x[0] for x in data])\n",
    "\n",
    "clf = LogisticRegression(penalty='l2')\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "scores = {\n",
    "    'acc': [],\n",
    "    'prec': [],\n",
    "    'rec': [],\n",
    "    'f1': [],\n",
    "    'auc': [],\n",
    "}\n",
    "\n",
    "# n by 2 vector, holding probabilities for 1's and 0's\n",
    "out_of_bag_probs = np.zeros((r.n_reviews, 2))\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_probs = clf.predict_proba(X_test)\n",
    "    \n",
    "    out_of_bag_probs[test_idx] = y_probs\n",
    "    \n",
    "    scores['acc'].append(accuracy_score(y_test, y_pred))\n",
    "    scores['prec'].append(precision_score(y_test, y_pred))\n",
    "    scores['rec'].append(recall_score(y_test, y_pred))\n",
    "    scores['f1'].append(f1_score(y_test, y_pred))\n",
    "    scores['auc'].append(roc_auc_score(y_test, y_probs[:,1]))\n",
    "    \n",
    "for metric, scores in scores.items():\n",
    "    print('{} = {}'.format(metric, np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ^ These results are pretty good\n",
    "\n",
    "As a comparison, the paper these data come from achieved a max accuracy of 87.2%. That paper had domain specific knowledge and a careful modeling procedure.\n",
    "\n",
    "We have some w2v code and no domain knowledge. We could imagine many improvements (doc vecs, tuning parameters, incorporating additional word-based signals, etc.).\n",
    "\n",
    "#### A simple way to increase accuracy is to increase `iter` in the w2v model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus content: plot the ROC curve\n",
    "\n",
    "Intuition: if we can get a predicted probability of being in each class, we can vary our cutoff (e.g. label `x > .3` as 1, `x > .4` as 1, etc.).\n",
    "\n",
    "When we vary the cutoff, the true positive rate (TPR) and false positive rate (FPR) of the 1's will change.\n",
    "\n",
    "At low cutoffs, we will get more of the true 1's, but will also have more 0's erroneously classified as 1's.\n",
    "\n",
    "AUC-ROC lets us visually investigate this and choose an optimal cutoff. It also lets us see how well our classifier performs across cutoff levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c888eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "%matplotlib inline\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y, out_of_bag_probs[:,1])\n",
    "auc = roc_auc_score(y, out_of_bag_probs[:,1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    color='darkorange',\n",
    "    lw=2,\n",
    "    label='ROC curve (area = {0:.3f})'.format(auc))\n",
    "\n",
    "plt.xlim([0., 1.])\n",
    "plt.ylim([0., 1.])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('AUC-ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
