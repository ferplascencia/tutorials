{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import gensim\n",
    "from random import sample\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "print('Fast version? (1 for yes) {}'.format(gensim.models.word2vec.FAST_VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Plan\n",
    "1. Class to read in data\n",
    "1. Train word embedding model\n",
    "1. Get ground truth\n",
    "1. Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ReviewGetter(object):\n",
    "    \"\"\"\n",
    "    This class iterates through reviews\n",
    "    \n",
    "    - Store input paths on the class\n",
    "    \n",
    "    - The __iter__ method, each time called, will run through the docs again\n",
    "      We need to iterate a bunch of times so this makes sense\n",
    "    \n",
    "    - On the first pass, we save the ground truth labels on the class\n",
    "    \n",
    "    - Also on first pass, we save word counts\n",
    "      Word probabilities will be important later\n",
    "    \"\"\"\n",
    "    BASE_FOLDER = 'review_polarity/txt_sentoken'\n",
    "    PATHS = {\n",
    "        'pos': glob.glob(BASE_FOLDER + '/pos/*.txt'),\n",
    "        'neg': glob.glob(BASE_FOLDER + '/neg/*.txt'),\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.n_reviews = 0\n",
    "        self.word_count_dict = {}\n",
    "        self.first_run = True\n",
    "    \n",
    "    def get_reviews(self):\n",
    "        \"\"\"\n",
    "        - Will iterate through the docs each time called\n",
    "        \n",
    "        - Yields a list of tokens\n",
    "        - We do one preprocessing step,\n",
    "          adding <b> at the beginning and </b> at the end of a review\n",
    "          plus adding <s> between sentences\n",
    "        \n",
    "        Example use:\n",
    "        r = ReviewIterator()\n",
    "        for review in r:\n",
    "            do_stuff(review)\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "\n",
    "        reviews = []\n",
    "        \n",
    "        for label, paths in self.PATHS.items():\n",
    "            if label == 'pos':\n",
    "                valence = 1\n",
    "            else:\n",
    "                valence = 0\n",
    "            for path in paths:             \n",
    "                with open(path, 'r') as infile:\n",
    "                    review = []\n",
    "                    for line in infile:\n",
    "                        review.extend(line.split() + ['<s>'])\n",
    "                        # bookkeeping\n",
    "                    count += 1\n",
    "                    if count % 500 == 0:\n",
    "                        logging.info('Yielded {} reviews'.format(count))\n",
    "                    if self.first_run == True:\n",
    "                        self.n_reviews += 1\n",
    "                        for word in review:\n",
    "                            try:\n",
    "                                self.word_count_dict[word] += 1\n",
    "                            except:\n",
    "                                self.word_count_dict[word] = 1\n",
    "                    reviews.append((valence, review))\n",
    "        self.first_run = False\n",
    "        return reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Simple diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "r = ReviewGetter()\n",
    "\n",
    "data = r.get_reviews()\n",
    "\n",
    "reviews = [x[1] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(reviews)\n",
    "print(reviews[1][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word2Vec happens here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(\n",
    "    sample(reviews, len(reviews)),\n",
    "    iter=30, # number of algorithm iterations\n",
    "    seed=42,\n",
    "    workers=2, # number of CPU cores to use\n",
    "    size=100, # length of each word vector\n",
    "    sample=1e-4, # if word has p>sample, downsample it\n",
    "    window=10, # window on sides of focal word\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## W2V diagnostics\n",
    "\n",
    "We should pick some words and make sure their synonyms make sense to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=['woman'])\n",
    "w2v_model.wv.most_similar(positive=['man'], negative=['king'])\n",
    "w2v_model.wv.most_similar(positive=['spielberg'])\n",
    "w2v_model.wv.most_similar(positive=['spielberg', 'tarantino'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Combine to make review vectors\n",
    "\n",
    "We use a really simple method:\n",
    "1. Choose an `alpha` level (lit indicates 0.001 to 0.0001, but for our task a value of 1 works best)\n",
    "1. For each word a person uses, get the word vector `v_word`\n",
    "1. Weight `v_word` by its overall frequency in the corpus `p_word` using formula `alpha / (alpha + p_word)` to get `v_word_weighted`. This downweights the impact of common words. A smaller `alpha` makes this downweighting more severe.\n",
    "1. Add `v_word_weighted` to the user vector `v_user` (which we initialize as all 0's.\n",
    "1. To account for different activity levels of different users, after we are done adding `v_word_weighted` for all words a user has used, we divide `v_user` by `n_words_user` to get our final user vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_vectors = w2v_model.wv\n",
    "\n",
    "word_vocab_dict = w2v_model.wv.vocab\n",
    "word_count_dict = r.word_count_dict\n",
    "\n",
    "word_prob_dict = {k: word_count_dict[k] for k in word_vocab_dict.keys()}\n",
    "word_count_sum = sum(word_prob_dict.values())\n",
    "word_prob_dict = {k: v / word_count_sum for k, v in word_prob_dict.items()}\n",
    "\n",
    "review_token_counts = np.zeros((2000,))\n",
    "review_vectors = np.zeros((2000, 100))\n",
    "\n",
    "a = 1\n",
    "\n",
    "for idx, (label, review_tokens) in enumerate(data):\n",
    "    for token in review_tokens:\n",
    "        if token in word_vectors:\n",
    "            p = word_prob_dict[token]\n",
    "            review_token_counts[idx] += 1\n",
    "            review_vectors[idx] += (a / (a + p)) * word_vectors[token]\n",
    "\n",
    "review_vectors = review_vectors / review_token_counts[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model and evaluate\n",
    "\n",
    "### Model\n",
    "\n",
    "Logistic regression with L2 penalty (\"Ridge\").\n",
    "\n",
    "This penalizes the L2-norm of the coefficient vector.\n",
    "\n",
    "L2 logistic regression is the most common way I've seen of summarizing word vectors. They seem to do well with \"dense vectors\" (e.g. not many 0's).\n",
    "\n",
    "### Metrics\n",
    "\n",
    "1. Accuracy: What fraction of predictions are correct?\n",
    "1. Precision: What fraction of classified 1's are true 1's?\n",
    "1. Recall: What fraction of true 1's are actually predicted 1's from the classifier?\n",
    "1. F1: Harmonic mean of Precision and Recall (`2 * (p*r) / (p + r)`)\n",
    "1. AUC: Area under the receiver operating characteristic (ROC) curve. We show a plot below because this is an important measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = review_vectors\n",
    "y = np.array([x[0] for x in data])\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "scores = {\n",
    "    'acc': [],\n",
    "    'prec': [],\n",
    "    'rec': [],\n",
    "    'f1': [],\n",
    "    'auc': [],\n",
    "}\n",
    "\n",
    "# n by 2 vector, holding probabilities for 1's and 0's\n",
    "out_of_bag_probs = np.zeros((r.n_reviews, 2))\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_probs = clf.predict_proba(X_test)\n",
    "    \n",
    "    out_of_bag_probs[test_idx] = y_probs\n",
    "    \n",
    "    scores['acc'].append(accuracy_score(y_test, y_pred))\n",
    "    scores['prec'].append(precision_score(y_test, y_pred))\n",
    "    scores['rec'].append(recall_score(y_test, y_pred))\n",
    "    scores['f1'].append(f1_score(y_test, y_pred))\n",
    "    scores['auc'].append(roc_auc_score(y_test, y_probs[:,1]))\n",
    "    \n",
    "for metric, scores in scores.items():\n",
    "    print('{} = {}'.format(metric, np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ^ These results are pretty good\n",
    "\n",
    "As a comparison, the paper these data come from achieved a max accuracy of 87.2%. That paper had domain specific knowledge and a careful modeling procedure.\n",
    "\n",
    "We have some w2v code and no domain knowledge. We could imagine many improvements (doc vecs, tuning parameters, incorporating additional word-based signals, etc.).\n",
    "\n",
    "#### A simple way to increase accuracy is to increase `iter` in the w2v model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Bonus content: plot the ROC curve\n",
    "\n",
    "Intuition: if we can get a predicted probability of being in each class, we can vary our cutoff (e.g. label `x > .3` as 1, `x > .4` as 1, etc.).\n",
    "\n",
    "When we vary the cutoff, the true positive rate (TPR) and false positive rate (FPR) of the 1's will change.\n",
    "\n",
    "At low cutoffs, we will get more of the true 1's, but will also have more 0's erroneously classified as 1's.\n",
    "\n",
    "AUC-ROC lets us visually investigate this and choose an optimal cutoff. It also lets us see how well our classifier performs across cutoff levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "%matplotlib inline\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y, out_of_bag_probs[:,1])\n",
    "auc = roc_auc_score(y, out_of_bag_probs[:,1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    color='darkorange',\n",
    "    lw=2,\n",
    "    label='ROC curve (area = {0:.3f})'.format(auc))\n",
    "\n",
    "plt.xlim([0., 1.])\n",
    "plt.ylim([0., 1.])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('AUC-ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
